# ðŸ¤— Hugging Face Transformers Basics

This repository provides a **collection of practical scripts** to explore Hugging Face's Transformers library.  
It covers the essential workflows such as model loading, tokenization, pipelines, dataset handling, training, and semantic search.  

---

## ðŸ“š Contents

### ðŸ”¹ Model Basics
- AutoModel & AutoModelForSequenceClassification  
- Saving and loading transformer models  
- Creating a custom transformer model  

### ðŸ”¹ Tokenization
- Introduction to Hugging Face Tokenizers  
- Advanced tokenization examples (custom vocab, batching)  

### ðŸ”¹ Pipelines
- Using prebuilt pipelines for inference  
- Pipeline variations with text classification, Q&A, etc.  
- Batched pipelines for performance  

### ðŸ”¹ Data Handling
- Working with Hugging Face Datasets  
- Streaming large datasets efficiently  
- Using Apache Arrow format with datasets  

### ðŸ”¹ Training
- First training loop example  
- Advanced training with custom loops  
- Accelerated training with `Accelerator`  
- Data preprocessing before training  

### ðŸ”¹ Generation
- Decoderâ€“Generator models for text generation  
- Asymmetric Semantic Search examples  

### ðŸ”¹ Miscellaneous
- DistilBERT example usage  
- Audio processing scripts  
- Utility helpers  

---

## Tools
```bash
# Install dependencies
pip install transformers datasets accelerate
